{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vda/anaconda3/envs/mwp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"/home/vda/Prophetnet/train.csv\")\n",
    "val = pd.read_csv(\"/home/vda/Prophetnet/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 11.6MB/s]\n",
      "/home/vda/anaconda3/envs/mwp/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Downloading model.safetensors: 100%|██████████| 1.57G/1.57G [02:44<00:00, 9.52MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 2.35MB/s]\n",
      "Downloading (…)prophetnet.tokenizer: 100%|██████████| 232k/232k [00:00<00:00, 40.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 440kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 661kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import ProphetNetForConditionalGeneration, ProphetNetTokenizer, AdamW\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "tokenizer = ProphetNetTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "\n",
    "# Define your fine-tuning data\n",
    "train_input_str = train.Body.tolist()\n",
    "train_target_str = train.Question.tolist()\n",
    "val_input_str = val.Body.tolist()\n",
    "val_target_str = val.Question.tolist()\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "train_input_ids = tokenizer(train_input_str, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "train_target_ids = tokenizer(train_target_str, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "val_input_ids = tokenizer(val_input_str, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "val_target_ids = tokenizer(val_target_str, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids[\"input_ids\"], train_target_ids[\"input_ids\"])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_input_ids[\"input_ids\"], val_target_ids[\"input_ids\"])\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30 # total 30 epochs\n",
    "warmup_steps = 1e2\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vda/anaconda3/envs/mwp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training...\n",
      "Epoch 1/30, Avg. Loss: 0.40420332888762156\n",
      "Training time:  97.40243101119995\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 7.221677303314209\n",
      "\n",
      "======== Epoch 2 / 30 ========\n",
      "Training...\n",
      "Epoch 2/30, Avg. Loss: 0.8883681527773539\n",
      "Training time:  93.68661952018738\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.92\n",
      "  Validation took: 7.129942893981934\n",
      "\n",
      "======== Epoch 3 / 30 ========\n",
      "Training...\n",
      "Epoch 3/30, Avg. Loss: 0.6809177947044373\n",
      "Training time:  93.56731104850769\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 7.10741400718689\n",
      "\n",
      "======== Epoch 4 / 30 ========\n",
      "Training...\n",
      "Epoch 4/30, Avg. Loss: 0.3372596748669942\n",
      "Training time:  93.68126583099365\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 7.1124351024627686\n",
      "\n",
      "======== Epoch 5 / 30 ========\n",
      "Training...\n",
      "Epoch 5/30, Avg. Loss: 0.22322243869304656\n",
      "Training time:  93.62625408172607\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 7.105046272277832\n",
      "\n",
      "======== Epoch 6 / 30 ========\n",
      "Training...\n",
      "Epoch 6/30, Avg. Loss: 0.17031128967801729\n",
      "Training time:  93.60147762298584\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.41\n",
      "  Validation took: 7.222224473953247\n",
      "\n",
      "======== Epoch 7 / 30 ========\n",
      "Training...\n",
      "Epoch 7/30, Avg. Loss: 0.143417994727691\n",
      "Training time:  93.55555367469788\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.42\n",
      "  Validation took: 7.226293087005615\n",
      "\n",
      "======== Epoch 8 / 30 ========\n",
      "Training...\n",
      "Epoch 8/30, Avg. Loss: 0.12360698337356249\n",
      "Training time:  93.59853148460388\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 7.132883310317993\n",
      "\n",
      "======== Epoch 9 / 30 ========\n",
      "Training...\n",
      "Epoch 9/30, Avg. Loss: 0.09762579972545306\n",
      "Training time:  93.61023998260498\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 7.137816905975342\n",
      "\n",
      "======== Epoch 10 / 30 ========\n",
      "Training...\n",
      "Epoch 10/30, Avg. Loss: 0.09246583864092826\n",
      "Training time:  93.63135766983032\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.42\n",
      "  Validation took: 7.141627073287964\n",
      "\n",
      "======== Epoch 11 / 30 ========\n",
      "Training...\n",
      "Epoch 11/30, Avg. Loss: 0.07561305252214273\n",
      "Training time:  93.45237064361572\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 7.100891590118408\n",
      "\n",
      "======== Epoch 12 / 30 ========\n",
      "Training...\n",
      "Epoch 12/30, Avg. Loss: 0.0745236196865638\n",
      "Training time:  93.31166362762451\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.41\n",
      "  Validation took: 7.138134002685547\n",
      "\n",
      "======== Epoch 13 / 30 ========\n",
      "Training...\n",
      "Epoch 13/30, Avg. Loss: 0.05873323713739713\n",
      "Training time:  93.33412575721741\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 7.145748138427734\n",
      "\n",
      "======== Epoch 14 / 30 ========\n",
      "Training...\n",
      "Epoch 14/30, Avg. Loss: 0.04847245077292124\n",
      "Training time:  93.28648853302002\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 7.235497236251831\n",
      "\n",
      "======== Epoch 15 / 30 ========\n",
      "Training...\n",
      "Epoch 15/30, Avg. Loss: 0.04961017680664857\n",
      "Training time:  93.23099899291992\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.43\n",
      "  Validation took: 7.223767995834351\n",
      "\n",
      "======== Epoch 16 / 30 ========\n",
      "Training...\n",
      "Epoch 16/30, Avg. Loss: 0.040467585666726036\n",
      "Training time:  93.25950694084167\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.44\n",
      "  Validation took: 7.105510950088501\n",
      "\n",
      "======== Epoch 17 / 30 ========\n",
      "Training...\n",
      "Epoch 17/30, Avg. Loss: 0.03701378537962834\n",
      "Training time:  93.23608589172363\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.44\n",
      "  Validation took: 7.176209926605225\n",
      "\n",
      "======== Epoch 18 / 30 ========\n",
      "Training...\n",
      "Epoch 18/30, Avg. Loss: 0.03305439041306575\n",
      "Training time:  93.24179697036743\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 7.122577905654907\n",
      "\n",
      "======== Epoch 19 / 30 ========\n",
      "Training...\n",
      "Epoch 19/30, Avg. Loss: 0.04517489435772101\n",
      "Training time:  93.21127843856812\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 7.137666940689087\n",
      "\n",
      "======== Epoch 20 / 30 ========\n",
      "Training...\n",
      "Epoch 20/30, Avg. Loss: 0.032390203153093655\n",
      "Training time:  93.42687129974365\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 7.139384031295776\n",
      "\n",
      "======== Epoch 21 / 30 ========\n",
      "Training...\n",
      "Epoch 21/30, Avg. Loss: 0.027639292639990647\n",
      "Training time:  93.62170195579529\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.46\n",
      "  Validation took: 7.178906202316284\n",
      "\n",
      "======== Epoch 22 / 30 ========\n",
      "Training...\n",
      "Epoch 22/30, Avg. Loss: 0.022749089741458495\n",
      "Training time:  93.64237213134766\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.47\n",
      "  Validation took: 7.152869939804077\n",
      "\n",
      "======== Epoch 23 / 30 ========\n",
      "Training...\n",
      "Epoch 23/30, Avg. Loss: 0.019167740031455955\n",
      "Training time:  93.62510633468628\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.47\n",
      "  Validation took: 7.197497367858887\n",
      "\n",
      "======== Epoch 24 / 30 ========\n",
      "Training...\n",
      "Epoch 24/30, Avg. Loss: 0.017957505617911616\n",
      "Training time:  93.60938143730164\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 7.1250996589660645\n",
      "\n",
      "======== Epoch 25 / 30 ========\n",
      "Training...\n",
      "Epoch 25/30, Avg. Loss: 0.0163549195881933\n",
      "Training time:  92.81395530700684\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 7.020041227340698\n",
      "\n",
      "======== Epoch 26 / 30 ========\n",
      "Training...\n",
      "Epoch 26/30, Avg. Loss: 0.015754775097593664\n",
      "Training time:  92.71677851676941\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.48\n",
      "  Validation took: 7.02778172492981\n",
      "\n",
      "======== Epoch 27 / 30 ========\n",
      "Training...\n",
      "Epoch 27/30, Avg. Loss: 0.01334744945478936\n",
      "Training time:  106.49345803260803\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 9.52805757522583\n",
      "\n",
      "======== Epoch 28 / 30 ========\n",
      "Training...\n",
      "Epoch 28/30, Avg. Loss: 0.012827945457150538\n",
      "Training time:  96.63587355613708\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 7.641916275024414\n",
      "\n",
      "======== Epoch 29 / 30 ========\n",
      "Training...\n",
      "Epoch 29/30, Avg. Loss: 0.011517401464904348\n",
      "Training time:  93.70381689071655\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 7.046876668930054\n",
      "\n",
      "======== Epoch 30 / 30 ========\n",
      "Training...\n",
      "Epoch 30/30, Avg. Loss: 0.011504735350608825\n",
      "Training time:  93.84844660758972\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.49\n",
      "  Validation took: 7.030625104904175\n",
      "0.4059306940436363 11\n",
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "best_loss = 99999999\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        model.zero_grad()  \n",
    "        input_ids, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model(input_ids, labels=labels).loss\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg. Loss: {avg_train_loss}\")\n",
    "    print(\"Training time: \", time.time() - t0)\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_loader:\n",
    "        \n",
    "        input_ids, labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            loss = model(input_ids, labels=labels).loss\n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    if best_loss > avg_val_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model = model\n",
    "        best_epoch = epoch\n",
    "\n",
    "    validation_time = time.time() - t0   \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "print(best_loss, best_epoch)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_prophetnet\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "best_model.save_pretrained(\"./best_fine_tuned_prophetnet\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gen = []\n",
    "\n",
    "for i in range(len(test.Body.tolist())):\n",
    "  input_str = test.Body.tolist()[i]\n",
    "  input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "  # Generate predictions\n",
    "  with torch.no_grad():\n",
    "      output_ids = model.generate(input_ids)\n",
    "  # Convert the output_ids to text\n",
    "  predicted_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  print(predicted_str)\n",
    "  predictions_gen.append(predicted_str)\n",
    "\n",
    "pd.DataFrame(predictions_gen).to_csv(\"prediction_svamp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions_gen).to_csv(\"prediction_svamp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gen = []\n",
    "\n",
    "for i in range(len(test.Body.tolist())):\n",
    "  input_str = test.Body.tolist()[i]\n",
    "  input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "  # Generate predictions\n",
    "  with torch.no_grad():\n",
    "      output_ids = best_model.generate(input_ids)\n",
    "  # Convert the output_ids to text\n",
    "  predicted_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  print(predicted_str)\n",
    "  predictions_gen.append(predicted_str)\n",
    "\n",
    "pd.DataFrame(predictions_gen).to_csv(\"best_prediction_svamp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs = pd.read_csv(\"best_prediction_svamp.csv\")[\"0\"].tolist()\n",
    "contexts = pd.read_csv(\"test.csv\").Body.tolist()\n",
    "\n",
    "for i in range(len(generated_outputs)):\n",
    "  print(contexts[i])\n",
    "  print(generated_outputs[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "seqs = []\n",
    "for line in generated_outputs: \n",
    "  seqs.append(word_tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def distinct(seqs):\n",
    "    \"\"\" Calculate intra/inter distinct 1/2. \"\"\"\n",
    "    batch_size = len(seqs)\n",
    "    intra_dist1, intra_dist2 = [], []\n",
    "    unigrams_all, bigrams_all = Counter(), Counter()\n",
    "    for seq in seqs:\n",
    "        unigrams = Counter(seq)\n",
    "        bigrams = Counter(zip(seq, seq[1:]))\n",
    "        intra_dist1.append((len(unigrams)+1e-12) / (len(seq)+1e-5))\n",
    "        intra_dist2.append((len(bigrams)+1e-12) / (max(0, len(seq)-1)+1e-5))\n",
    "\n",
    "        unigrams_all.update(unigrams)\n",
    "        bigrams_all.update(bigrams)\n",
    "\n",
    "    inter_dist1 = (len(unigrams_all)+1e-12) / (sum(unigrams_all.values())+1e-5)\n",
    "    inter_dist2 = (len(bigrams_all)+1e-12) / (sum(bigrams_all.values())+1e-5)\n",
    "    intra_dist1 = np.average(intra_dist1)\n",
    "    intra_dist2 = np.average(intra_dist2)\n",
    "    return intra_dist1, intra_dist2, inter_dist1, inter_dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dis1:  0.1634808844895333\n",
      "dis2:  0.39597315214780177\n"
     ]
    }
   ],
   "source": [
    "_, _, dis1, dis2 = distinct(seqs)\n",
    "\n",
    "print(\"dis1: \", dis1)\n",
    "print(\"dis2: \", dis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "bert_score = 0\n",
    "\n",
    "cnt = 0\n",
    "assert len(generated_outputs) == len(contexts)\n",
    "rouge_scr = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx in tqdm(range(len(generated_outputs))):\n",
    "  generated_output = generated_outputs[idx]\n",
    "  context = contexts[idx]\n",
    "\n",
    "  _,_, bertscoreF1 = score([generated_output], [context], lang='en', verbose=True)\n",
    "  bert_score += float(bertscoreF1.mean())\n",
    "\n",
    "  cnt += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"BERTScore Relevancy: \", bert_score/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Relevancy:  0.8800921764969826\n"
     ]
    }
   ],
   "source": [
    "print(\"BERTScore Relevancy: \", bert_score/cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
